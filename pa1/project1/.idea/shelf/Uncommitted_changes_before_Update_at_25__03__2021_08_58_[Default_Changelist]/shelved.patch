Index: crawler.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import hashlib\r\nimport os\r\nimport threading\r\nimport time\r\nfrom urllib.request import urlopen, Request\r\nfrom datetime import datetime\r\nimport requests\r\nfrom url_normalize import url_normalize\r\n\r\nimport urllib.robotparser\r\nfrom bs4 import BeautifulSoup\r\n\r\nfrom urllib.error import HTTPError\r\nfrom urllib.parse import urlparse, urlsplit, urljoin\r\nimport urllib\r\n\r\n\r\nclass MainCrawler:\r\n\r\n    def __init__(self, db=None, fr=None, robotPages=None, thisIsCrawlerNumber=None):\r\n        self.db = db\r\n        self.fr = fr\r\n        self.robotPages = robotPages\r\n        self.thisIsCrawlerNumber = thisIsCrawlerNumber\r\n\r\n    def startThread(self):\r\n        self.thread = threading.Thread(target=self.mainFunction)\r\n        self.thread.setDaemon(True)\r\n        self.thread.start()\r\n        return self.thread\r\n\r\n    def timePassed(self, prevTime):\r\n        return time.time() - prevTime >= 5\r\n\r\n    def canonicalUrl(self, url):\r\n        splited = '{uri.scheme}://{uri.netloc}/'.format(uri=urlsplit(url))\r\n        return url_normalize(urljoin(splited, url))\r\n\r\n    def takeAllRobotPages(self, robotText, domain):\r\n        a = robotText.split(\"\\n\")[0]\r\n        if \"*\" in a or \"fri-wier-obidzuko\" in a:\r\n            for line in robotText.split(\"\\n\"):\r\n                if line.startswith('Disallow'):  # this is for disallowed url\r\n                    self.robotPages.append('http://' + domain + line.split(': ')[1].split(' ')[0])\r\n\r\n    def mainFunction(self):\r\n        currentPageLink = self.fr.getUrl()\r\n        currentTime = time.time()\r\n\r\n        while currentPageLink is not None:\r\n            print(str(self.thisIsCrawlerNumber) + ', CURRENT PAGE: ' + currentPageLink[0])\r\n            if self.timePassed(currentTime):\r\n                time.sleep(1)\r\n\r\n            # probaj da ja otvoris narednata strana so e na red\r\n            try:\r\n                f = urlopen(Request(currentPageLink[0], headers={'User-Agent': 'fri-wier-obidzuko'}), timeout=10)\r\n                currentTime = time.time()\r\n            except HTTPError as httperror:\r\n                print(str(self.thisIsCrawlerNumber) + ', STATUS CODE ERROR !!!!!!')\r\n                print(str(self.thisIsCrawlerNumber) + str(httperror.getcode()))\r\n                pageId = self.db.insertPage(None, None, currentPageLink[0], None, httperror.getcode(), datetime.now(), None)\r\n                self.db.insertLink(currentPageLink[1], pageId)\r\n                currentPageLink = self.fr.getUrl()\r\n                continue\r\n            except Exception:\r\n                # vo slucaj da e nekoj los link, zemame link od druga strana i odime od pocetok\r\n                print(str(self.thisIsCrawlerNumber) + ', ERROR: THIS PAGE DOES NOT EXIST')\r\n                currentPageLink = self.fr.getUrl()\r\n                continue\r\n\r\n            # ova mora zaradi preusmeruvanje, koga sme preusmereni proveruvame na koj link sme sega\r\n            # ako sme preusmereni ova ce go daj tocnio, toj koj so se koristi, i se e bez problem\r\n            currentPageLink[0] = f.url\r\n\r\n            print(str(self.thisIsCrawlerNumber) + ', CHANGED PAGE: ' + currentPageLink[0])\r\n            # ako e zabraneto (robots.txt) zemi naredna strana\r\n            if currentPageLink[0] in self.robotPages:\r\n                currentPageLink = self.fr.getUrl()\r\n                continue\r\n\r\n            # ovoj url veke go imame vo bazata => zemi nareden\r\n            if self.db.getPageByUrl(self.canonicalUrl(currentPageLink[0])) is not None:\r\n                currentPageLink = self.fr.getUrl()\r\n                continue\r\n\r\n            if '.zip' in currentPageLink[0]:\r\n                currentPageLink = self.fr.getUrl()\r\n                continue\r\n\r\n            domain = urlparse(currentPageLink[0]).netloc # dava primer www.gov.si -> mora https://......../pomoc/\r\n            if \".gov.si\" not in domain:\r\n                currentPageLink = self.fr.getUrl()\r\n                continue\r\n            print(str(self.thisIsCrawlerNumber) + ', DOMAIN: ' + domain)\r\n\r\n            info = f.info()\r\n            page_type_code = info.get_content_type()\r\n            print(str(self.thisIsCrawlerNumber) + page_type_code)\r\n            htmlStatusCode = f.getcode()\r\n            if page_type_code == 'text/html':\r\n                page = f.read().decode('utf-8')\r\n                soup = BeautifulSoup(page)\r\n                html_content = page\r\n                hash_object = hashlib.sha256(html_content.encode())\r\n                html_hash = hash_object.hexdigest()\r\n\r\n                # gledame dali toj page e duplikat\r\n                hashPageId = self.db.getPageByHash(html_hash)\r\n            else:\r\n                hashPageId = None\r\n                html_content = None\r\n                html_hash = None\r\n\r\n            # gledame dali sme na istiot domain, ako ne sme => dodadi nov site\r\n            siteID = self.db.getSiteByDomain(domain)\r\n            if siteID is None:\r\n                # procitaj go robot.txt na toj site\r\n                robotURL = 'http://' + domain + '/robots.txt'\r\n                robotFile = urllib.robotparser.RobotFileParser()\r\n                robotFile.set_url(robotURL)\r\n                robotText = None\r\n                siteText = None\r\n                try:\r\n                    robotFile.read()\r\n                    if robotFile.default_entry:\r\n                        robotText = str(robotFile.default_entry)\r\n                        self.takeAllRobotPages(robotText, domain)\r\n                        print(str(self.thisIsCrawlerNumber) + self.robotPages)\r\n                    if robotFile.site_maps():\r\n                        siteText = str(\"\\n\".join(robotFile.site_maps()))\r\n                except Exception:\r\n                    robotText = None\r\n                    siteText = None\r\n                    print(str(self.thisIsCrawlerNumber) + ', EXCEPTION WHILE CREATING ROBOT')\r\n\r\n                siteID = self.db.insertSite(domain, robotText, siteText)\r\n\r\n            if hashPageId is None:\r\n                if 'text/html' in page_type_code:\r\n                    page_type_code = 'HTML'\r\n                else:\r\n                    page_type_code = 'BINARY'\r\n                    request_headers = requests.utils.default_headers()\r\n                    request_headers.update(\r\n                        {\"User-Agent\": \"fri-wier-obidzuko\"}\r\n                    )\r\n                    response = requests.head(currentPageLink[0], headers=request_headers)\r\n                    headers = response.headers\r\n                    content_type_headers = headers.get('content-type')\r\n                    content_type = \"/\"\r\n                    if content_type_headers == 'application/vnd.ms-powerpoint':\r\n                        content_type = 'PPT'\r\n                    elif content_type_headers == 'application/vnd.openxmlformats-officedocument.presentationml.presentation':\r\n                        content_type = 'PPTX'\r\n                    elif content_type_headers == 'application/msword':\r\n                        content_type = 'DOC'\r\n                    elif content_type_headers == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':\r\n                        content_type = 'DOCX'\r\n                    elif content_type_headers == 'application/pdf':\r\n                        content_type = 'PDF'\r\n                pageID = self.db.insertPage(siteID, page_type_code, self.canonicalUrl(currentPageLink[0]), html_content,\r\n                                       htmlStatusCode, datetime.now(), html_hash)\r\n                if currentPageLink[1] != 0:\r\n                    self.db.insertLink(currentPageLink[1], pageID)\r\n                if page_type_code == 'BINARY':\r\n                    if content_type != '/':\r\n                        self.db.insertPageData(pageID, content_type)\r\n                    currentPageLink = self.fr.getUrl()\r\n                    continue\r\n            else:\r\n                print(str(self.thisIsCrawlerNumber) + ', ' + str(hashPageId))\r\n                pageID = self.db.insertPage(siteID, 'DUPLICATE', self.canonicalUrl(currentPageLink[0]), html_content,\r\n                                       htmlStatusCode, datetime.now(), html_hash)\r\n                if currentPageLink[1] != 0:\r\n                    self.db.insertLink(currentPageLink[1], pageID)\r\n                self.db.insertLink(pageID, hashPageId)\r\n                currentPageLink = self.fr.getUrl()\r\n                continue\r\n\r\n            linkovi = soup.find_all('a', href=True)\r\n            sliki = soup.find_all('img', src=True)\r\n\r\n            # ovaj for e za linkovi\r\n            for lnk in linkovi: ################## smeni da gi pomini site ############################\r\n                # if the link is not empty add the link to the database\r\n                if '.jpg' in lnk['href'] or '.png' in lnk['href'] or '.jpeg' in lnk['href'] or '.svg' in lnk['href'] \\\r\n                        or '.gif' in lnk['href'] or '.avif' in lnk['href'] or '.apng' in lnk['href'] \\\r\n                        or '.wbep' in lnk['href'] or '.pjp' in lnk['href'] or '.jfif' in lnk['href']:\r\n                    continue\r\n                if lnk['href'] != '/':\r\n                    if (lnk['href']).startswith('http'):\r\n                        self.fr.addUrl(lnk['href'], pageID)\r\n                    else:\r\n                        # 'https://' + 'www.gov.si' + '/pomoc/\r\n                        self.fr.addUrl('http://' + domain + lnk['href'], pageID)\r\n\r\n            # ovaj for e za sliki\r\n            for sl in sliki[1:10]: ################## smeni da gi pomini site ############################\r\n                if sl['src'] != '/': # if the img is not empty add the img to the database\r\n                    if (sl['src']).startswith('http') or (sl['src']).startswith('data'):\r\n                        pictureLink = sl['src']\r\n                    elif (sl['src']).startswith('/' + domain):\r\n                        pictureLink = 'http:/' + sl['src']\r\n                    elif not (sl['src']).startswith('/'):\r\n                        pictureLink = 'http://' + domain + '/' + sl['src']\r\n                    else:\r\n                        # 'https://' + 'www.gov.si' + '/pomoc/\r\n                        pictureLink = 'http://' + domain + sl['src']\r\n\r\n                    print(str(self.thisIsCrawlerNumber) + pictureLink)\r\n                    a = urlparse(pictureLink)\r\n                    filename = os.path.basename(a.path)\r\n\r\n                    content_type = \"/\"\r\n                    if filename.__contains__('.'):\r\n                        content_type_table = filename.split(\".\")\r\n                        content_type = content_type_table[len(content_type_table)-1]\r\n\r\n                    try:\r\n                        data = urlopen(pictureLink).read()\r\n                        self.db.insertImage(pageID, filename, content_type, data, datetime.now())\r\n                    except Exception:\r\n                        print(str(self.thisIsCrawlerNumber) + ', TIMEOUT ERROR OR SKIPPED A PICTURE WITH A BAD URL')\r\n\r\n            currentPageLink = self.fr.getUrl() # ova posledno za da zemi strana od pocetoko\r\n\r\n\r\n\r\n\r\n###################### STA NAMA OBIDZUKOVCI FALI (OSIM MOZAK I NERVE) ##########################\r\n# 1. status code (page tabela) - ne raboti ko ce e 404 error i taka natamu  (PITAJ ASISTENT)\r\n# 2. (luksuz) povekje roboti da rabotat istovremeno  // NE E NAPRAVENO\r\n# 3. (luksuz) za da ne go preopteretuvame servero TIMEOUT  // VALJDA E OK\r\n# 4. (luksuz) agent so imeto obidzuko   OK\r\n\r\n######################## prasanja koi ne' macat ########################\r\n# 1. Error so imame ako moze da se resi\r\n# 2. Status code imame samo 200, dali e ok, dali treba drugite da se zacuvat?\r\n# 3. Binary nemame seuste nikade....\r\n# 4. Kolku roboti e pametno da se napravat koga ke se napravi konecen run\r\n# 5. So ako dojdi zip link? Dali da se preripuva?
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/crawler.py b/crawler.py
--- a/crawler.py	(revision c7b6267af8c9a111f556c80b53f6d1bb9a41e8c4)
+++ b/crawler.py	(date 1616658789941)
@@ -50,6 +50,7 @@
         while currentPageLink is not None:
             print(str(self.thisIsCrawlerNumber) + ', CURRENT PAGE: ' + currentPageLink[0])
             if self.timePassed(currentTime):
+                print("pocekaj")
                 time.sleep(1)
 
             # probaj da ja otvoris narednata strana so e na red
